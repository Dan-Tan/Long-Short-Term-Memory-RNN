# -*- coding: utf-8 -*-
"""LSTM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1haNhNnajNNXDFKs3NiBftRq-fO6PNHt3
"""

import numpy as np
import pandas as pd
import math
from google.colab import drive
import copy
import matplotlib.pyplot as plt
#drive.mount('/content/drive')

#activations and derivatives
def sigmoid(z):
    return (1/(1 + np.exp(-z)))

def tanh(z):
    return np.tanh(z)

def reLu(z):
    z[z<0] = 0
    return z

def reLu_prime(z):
    z[z < 0] = 0
    z[z > 0] = 1
    return z

def tanh_prime(z):
    return 1 - np.tanh(z)**2

def sigmoid_prime(z):
    return sigmoid(z) * (1 - sigmoid(z))

class FC_Layer():

    def __init__(self, input_size, hidden_size, output_size, activation, a_prime, learning_rate, momentum):

        self.input_size = input_size # for the architecture of the layers
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.activation = activation
        self.a_prime = a_prime

        self.learning_rate = learning_rate #for backpropagtion
        self.momentum = momentum

        self.W1 = np.random.normal(0, 1, (input_size, hidden_size)) # model parameters
        self.B1 = np.random.normal(0, 1, (hidden_size,))
        self.W2 = np.random.normal(0, 1, (hidden_size, output_size))
        self.B2 = np.random.normal(0, 1, (output_size,))

        self.z1 = []
        self.a1 = []
        self.z2 = []
        self.a2 = []
        self.inputs = []

        self.truth = False

        self.dLdW2, self.dLdW1, self.dLdB2, self.dLdB1 = (0, 0, 0, 0)

    def addToList(self): #to dynamically increase the size of lists
        self.z1.append(0)
        self.a1.append(0)
        self.z2.append(0)
        self.a2.append(0)

    def forward_prop(self, inputs, t):

        self.inputs.append(inputs)

        # print("W1_s: %s W2_s: %s in_s: %s" % (self.W1.shape, self.W2.shape, inputs.shape))

        self.addToList()

        self.z1[t] = np.dot(inputs, self.W1) + self.B1
        self.a1[t] = sigmoid(self.z1[t])
        self.z2[t] = np.dot(self.a1[t], self.W2) + self.B2
        self.a2[t] = self.activation(self.z2[t])

        if (self.truth):
            print("in: %s, z1: %s, a1: %s, z2: %s, a2: %s" % (inputs[0], self.z1[t][0], self.a1[t][0], self.z2[t][0], self.a2[t][0]))

        return self.a2[t] 

    def update_der(self, dLdW1, dLdW2, dLdB1, dLdB2):

        self.dLdW2 += dLdW2
        self.dLdW1 += dLdW1
        self.dLdB2 += dLdB2
        self.dLdB1 += dLdB1

    def update_params(self):

        self.W1 -= self.learning_rate * self.dLdW1
        self.W2 -= self.learning_rate * self.dLdW2
        self.B1 -= self.learning_rate * self.dLdB1
        self.B2 -= self.learning_rate * self.dLdB2 # needs to be changed to incooperate momentum

    def back_prop(self, dEdO, t):
        

        delta2 = dEdO * self.a_prime(self.z2[t])
        dLdW2 = np.dot(self.a1[t].T.reshape((self.hidden_size, 1)), delta2.reshape((1, self.output_size)))

        delta1 = np.dot(delta2, self.W2.T) * sigmoid_prime(self.z1[t])
        dLdW1 = np.dot(self.inputs[t].T.reshape((self.input_size, 1)), delta1.reshape(1, self.hidden_size))

        dLdX = np.dot(delta1, self.W1.T)

        self.dLdX = dLdX

        if (self.truth):
            print("dEdO: %s" % (dEdO,))

            print("delta2: %s" % (delta2,))
            print("dLdW2: %s" % (dLdW2[0],))
            print("delta1: %s" % (delta1,))
            print("dLdW1: %s" % (dLdW1[0],))
            print("dLdX: %s" % (dLdX,))

            print("z1: %s, a1: %s, z2: %s, a2: %s" % (self.z1[t], self.a1[t], self.z2[t], self.a2[t]))

        return dLdW1, dLdW2, delta1, delta2, dLdX # the delta is bias grads

#gradient testing

np.random.seed(6)
Test = FC_Layer(1, 1, 1, tanh, tanh_prime, 0.001, 0.8)
np.random.seed(6)
Test2 = FC_Layer(1, 1, 1, tanh, tanh_prime, 0.001, 0.8)

test_input = np.random.normal(0, 2, (1,))

test_output = np.random.normal(0, 2, (1,))

pred = Test.forward_prop(test_input, 0)

Test2.W2[0] += 0.00001

pred2 = Test2.forward_prop(test_input, 0)

a, b, c, d, dLdX = Test.back_prop(-(test_output - pred), 0)
Test.update_der(a, b, c, d)

error = ((test_output - pred)**2).sum()
error2 = ((test_output - pred2)**2).sum()

print((error2 - error)/(0.00001*2))
print(Test.dLdW2[0])

class LSTM_Layer():

    def __init__(self, input_size, hidden_size, mem_size, forget_memsize, forget_insize, inputhd_size, outputhd_size, learning_rate, momentum): #all ints

        self.input_size = input_size
        self.hidden_size = hidden_size

        self.fmem = FC_Layer(hidden_size + input_size, forget_insize, mem_size, sigmoid, sigmoid_prime, learning_rate, momentum)
        self.finnet = FC_Layer(hidden_size + input_size, forget_insize, mem_size, sigmoid, sigmoid_prime, learning_rate, momentum)
        self.inputnet = FC_Layer(hidden_size + input_size, inputhd_size, mem_size, tanh, tanh_prime, learning_rate, momentum)
        self.outputfnet = FC_Layer(hidden_size + input_size, outputhd_size, hidden_size, sigmoid, sigmoid_prime, learning_rate, momentum)
        self.outputnet = FC_Layer(mem_size, outputhd_size, hidden_size, tanh, tanh_prime, learning_rate, momentum)

        self.memory = []
        self.memory_gate = []
        self.input_gate = []
        self.input_tan = []
        self.outputf_gate = []
        self.output_gate = []
        self.hidden_state = []

        self.truth = False

        self.memory.append(np.random.normal(0, 1, (input_size, )))
        self.hidden_state.append(np.random.normal(0, 1, (hidden_size, )))


    def forward_prop(self, inputs, t): # full forward prop of the cell

        if (self.truth): #for debugging purposes
            self.outputnet.truth = True
            self.outputfnet.truth = True
            self.finnet.truth = True
            self.inputnet.truth = True
            self.fmem.truth = True

        self.inputs = inputs

        self.conc_in = np.concatenate((inputs, self.hidden_state[t]), 0)


        self.memory_gate.append(self.fmem.forward_prop(self.conc_in, t))
        self.memory.append(self.memory[t] * self.memory_gate[t])

        self.input_gate.append(self.finnet.forward_prop(self.conc_in, t))
        self.input_tan.append(self.inputnet.forward_prop(self.conc_in, t))

        self.memory[t+1] += np.multiply(self.input_gate[t], self.input_tan[t])

        if (self.truth):
            print("outputf_gate")
        self.outputf_gate.append(self.outputfnet.forward_prop(self.conc_in, t))
        if (self.truth):
            print("output_gate")
        self.output_gate.append(self.outputnet.forward_prop(self.memory[t+1], t))

        self.hidden_state.append((np.multiply(self.output_gate[t].reshape((self.hidden_size,)), self.outputf_gate[t])).reshape((self.hidden_size,)))

        return self.hidden_state[t+1]

        # dEdCp should be 0 if this is the first back_prop iteration

    def calc_grads(self, dEdHe, dEdCp, t): # this function should be called after each forward propagation as value will be overwritten

        if (self.truth):
            print("----outputnet----")
        dEdW1E, dEdW2E, dEdB1E, dEdB2E, dEdC = self.outputnet.back_prop(dEdHe * self.outputfnet.a2[t], t)
        self.outputnet.update_der(dEdW1E, dEdW2E, dEdB1E, dEdB2E)

        dEdC += dEdCp # adding the first path of the derivative 

        if (self.truth):
            print("----outputfnet----")
        dEdW1D, dEdW2D, dEdB1D, dEdB2D, dEdXD = self.outputfnet.back_prop(dEdHe * self.outputnet.a2[t], t)
        self.outputfnet.update_der(dEdW1D, dEdW2D, dEdB1D, dEdB2D)

        if (self.truth):
            print("----inputnet----")
        dEdW1C, dEdW2C, dEdB1C, dEdB2C, dEdXC = self.inputnet.back_prop(dEdC * self.finnet.a2[t], t)
        self.inputnet.update_der(dEdW1C, dEdW2C, dEdB1C, dEdB2C)

        if (self.truth):
            print("----finnet----")
        dEdW1B, dEdW2B, dEdB1B, dEdB2B, dEdXB = self.finnet.back_prop(dEdC * self.inputnet.a2[t], t)

        print("dEdMem: %s" % (dEdHe * dEdC * self.outputfnet.a2[t]))

        if (self.truth):
            print("----fmem----")
        dEdW1A, dEdW2A, dEdB1A, dEdB2A, dEdXA = self.fmem.back_prop(dEdC * self.memory[t], t)
        self.finnet.update_der(dEdW1B, dEdW2B, dEdB1B, dEdB2B)

        self.fmem.update_der(dEdW1A, dEdW2A, dEdB1A, dEdB2A)
        
        dEdCo = dEdC * (np.multiply(self.input_gate[t], self.input_tan[t]) * self.fmem.a2[t]) # only calculate single pathway full derivative is calculated in the preceding cell as we require the back prop there (needs dEdX to back pushed back through outputnet <t-1>)
            #this will be dEdCp for the next iteration for <t-1>

        dEdX = dEdXA + dEdXB + dEdXC + dEdXD

        return dEdCo, dEdX # dEdX requires reshaping due to the concatenation in forward prop.

#numerical graident testing testing
#note for singleton testing its easier to not use reLu activation for debugging as you will not encounter 0 gradients which create many false positives

np.random.seed(15)
LSTest = LSTM_Layer(10, 10, 10, 10, 10, 10, 10, 0.01, 0.8)
np.random.seed(15)
LSTest2 = LSTM_Layer(10, 10, 10, 10, 10, 10, 10, 0.01, 0.8)


# np.random.seed(5)
# LSTest = LSTM_Layer(1, 1, 1, 1, 1, 1, 1, 0.01, 0.8)
# np.random.seed(5)
# LSTest2 = LSTM_Layer(1, 1, 1, 1, 1, 1, 1, 0.01, 0.8)
test_input = np.random.normal(0, 1, (10,))

test_output = np.random.normal(0, 1, (10,))

LSTest2.truth = True
LSTest.truth = True

pred = LSTest.forward_prop(test_input, 0)
print("-----------------------")

# test_input[0] += 0.0000001
LSTest2.inputnet.W2[0][0] += 0.0000001

pred2 = LSTest2.forward_prop(test_input, 0)


print("pred: %s" % (pred))
print("pred2: %s" % (pred2))

dEdCo, dEdX = LSTest.calc_grads(-(test_output - pred), 0, 0)

error = ((test_output - pred)**2).sum()
error2 = ((test_output - pred2)**2).sum()

aprox = (error2 - error)/(0.0000001 * 2)
print(dEdX)
print(aprox)

class LSTM_Net():

    def __init__(self, cell1, cell2, fc, num_lookback): # lstm1 lstm2 fully_connected

        self.cell1 = cell1
        self.cell2 = cell2
        self.fc = fc
        self.truth = False

        self.num_lookback = num_lookback

    def prep_inputs(self, time_series):
        self.num_steps = time_series.shape[0] - self.num_lookback + 1
        inp_array = np.zeros((self.num_steps, self.cell1.input_size))

        for i in range(self.num_steps):
            inp_array[i] = time_series[i:i+self.num_lookback]

        return inp_array

    def forward_prop(self, time_series):

        if (self.truth):
            self.cell1.truth = True
            self.cell2.truth = True
            self.fc.truth = True
        
        inputs = self.prep_inputs(time_series)

        for i in range(self.num_steps):
            out1 = self.cell1.forward_prop(inputs[i], i)
            out2 = self.cell2.forward_prop(out1, i)
            fin_out = self.fc.forward_prop(out2, i)
            print("out_above")

            # self.back_prop(time_series[i+1], fin_out, i)
        return fin_out

    def back_prop(self, gtruth, output, t): # we use stochastic gradient descent (easiest to implement :D )

        dEdO = (-1) * (gtruth[0] - output[0])
        print(dEdO)

        fcdLdW1, fcdLdW2, fcdLdB1, fcdLdB2, dLdX = self.fc.back_prop(dEdO, 0)
        self.fc.update_der(fcdLdW1, fcdLdW2, fcdLdB1, fcdLdB2)
        c2_dEdCo, c2_dEdX = self.cell2.calc_grads(dLdX, 0, 0)
        c1_dEdO = c2_dEdX[:self.cell2.input_size]
        c2_dEdX = c2_dEdX[self.cell2.input_size:]
        c1_dEdCo, c1_dEdX = self.cell1.calc_grads(c1_dEdO, 0, 0)
        c1_dEdX = c1_dEdX[self.cell1.input_size:]
        counter = 1
        while counter <= t:
            dEdO = (-1) * (gtruth[counter] - output[counter])
            fcdLdW1, fcdLdW2, fcdLdB1, fcdLdB2, dLdX = self.fc.back_prop(dEdO, counter)
            self.fc.update_der(fcdLdW1, fcdLdW2, fcdLdB1, fcdLdB2)
            c2_dEdCo, c2_dEdX = self.cell2.calc_grads(c2_dEdX, c2_dEdCo, counter)
            c1_dEdO = c2_dEdX[:self.cell2.input_size]
            c2_dEdX = c2_dEdX[self.cell2.input_size:]
            c1_dEdCo, c1_dEdX = self.cell1.calc_grads(c1_dEdX+c1_dEdO, c1_dEdCo, counter)
            counter += 1

        print(c1_dEdX.shape)

        return c1_dEdX #returns param to make gradient checking easier
            
    def update_params(self):
        cell2.update_params()
        cell1.update_params()
        fc.update_params()

#numerical gradient testing

np.random.seed(3)
cell1 = LSTM_Layer(1, 1, 1, 1, 1, 1, 1, 0.01, 0.8)
cell2 = LSTM_Layer(1, 1, 1, 1, 1, 1, 1, 0.01, 0.8)
Fc = FC_Layer(1, 1, 1, sigmoid, sigmoid_prime, 0.01, 0.8)
LSTM = LSTM_Net(cell1, cell2, Fc, 1)

np.random.seed(3)
cell1n = LSTM_Layer(1, 1, 1, 1, 1, 1, 1, 0.01, 0.8)
cell2n = LSTM_Layer(1, 1, 1, 1, 1, 1, 1, 0.01, 0.8)
Fcn = FC_Layer(1, 1, 1, sigmoid, sigmoid_prime, 0.01, 0.8)
LSTMn = LSTM_Net(cell1n, cell2n, Fcn, 1)

LSTM.truth = True
LSTMn.truth = True

test_input = np.random.normal(0, 1, (1,1))
test_output = np.random.normal(0, 1, (1,1))

pred = LSTM.forward_prop(test_input)
LSTMn.cell2.outputnet.W2[0][0] += 0.00000001
pred_eps = LSTMn.forward_prop(test_input)
print("Here")
print(-(test_output[0] - pred[0]))
error = ((test_output[0] - pred)**2).sum()

LSTM.back_prop(test_output[0], pred, 0)

error_eps = ((test_output[0] - pred_eps)**2).sum()

approx = (error_eps - error)/0.00000002
print(approx)
print(LSTM.cell2.outputnet.dLdW2)